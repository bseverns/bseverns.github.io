cards:
  - id: facetimes
    title: Human-Buffer — Consent-Forward Vision Station
    img_src: /assets/images/cds/faceTimes-consent.svg
    img_alt: Human-Buffer consent screen showing detection-only, opt-in language and a visible delete-now action
    abstract: >-
      On-device, detection-only pipeline with explicit opt-in and delete-now controls.
      No identification, no network calls, and no storage without consent. Interface copy
      foregrounds agency before computation; documentation (README, PRIVACY/ETHICS,
      assumption ledger) is treated as research output.
    abstract_locked: true
    aligns: ["digital literacy","justice/representation","civic engagement","DIY"]
    methods:
      - Classroom demo & public kiosk contexts
      - Detection overlay with visible status + erase
      - "System: webcam → local detector → ephemeral buffer → user action"
    outcomes:
      - v2.1 release adds the ConsentDetect teaching sketch + auto-purge (CHANGELOG)
      - v2 shipped consent-toggle long press, session review, and double-press save (CHANGELOG)
      - Published assumption ledger covering detector defaults and purge schedule
      - Documented 2023→2025 development lineage for critique context
    teach:
      goal: Practice consent-forward capture as digital literacy
      lab60: Implement detection-only pipeline; test erase-now; reflect on affordances
      assess: Process log + assumption ledger + 2-min demo
    links:
      - label: Repository
        url: https://github.com/bseverns/Human-Buffer
      - label: Assumption ledger
        url: https://github.com/bseverns/Human-Buffer/blob/main/docs/assumption-ledger.md
      - label: Logic Diagrams
        url: https://github.com/bseverns/Human-Buffer/blob/main/diagrams.md
      - label: Lineage
        url: https://github.com/bseverns/Human-Buffer/blob/main/docs/lineage.md
      - label: Related sketches — videoProcessing laboratory notes
        url: https://github.com/bseverns/videoProcessing
      - label: Vimeo clip
        url: https://vimeo.com/1120504220
      - label: Privacy and Ethics in my practice
        url: https://bseverns.github.io/PRIVACY_ETHICS.md

  - id: moarknobs-42
    title: MOARkNOBS-42 — open-source microcontroller MIDI controller
    img_src: /assets/images/cds/mn42-panel.png
    img_alt: MOARkNOBS-42 panel top-down with labeled controls
    abstract: >-
      Reproducible hardware + firmware used as both instrument and teaching platform.
      Parameter mapping functions as an inquiry into authorship and control; latency is
      characterized and documented so performance claims are auditable and extendable.
    abstract_locked: true
    aligns: ["digital literacy","civic engagement","DIY"]
    methods:
      - Teensy-based instrument + teaching rig
      - "Complete docs: BOM, wiring, parameter map"
      - Measurement folded into practice (latency notes)
    outcomes:
      - "[Latency characterization lab (draft) published](/research/mn42-latency-lab/)"
      - Replication course module planned Spring 2026.
    teach:
      goal: Interface as authorship; documentation as scholarship
      lab60: Map controls → parameters; measure + log latency
      assess: Short demo + latency note + reproducibility checklist
    links:
      - label: Repository
        url: https://github.com/bseverns/MOARkNOBS-42
      - label: Release (latest)
        url: https://github.com/bseverns/MOARkNOBS-42/releases/latest
      - label: TESTING.md
        url: https://github.com/bseverns/MOARkNOBS-42/blob/main/docs/TESTING.md
      - label: PinMap.md
        url: https://github.com/bseverns/MOARkNOBS-42/blob/main/docs/PinMap.md
      - label: hardware/
        url: https://github.com/bseverns/MOARkNOBS-42/tree/main/hardware
      - label: Related sketch — benzknob
        url: https://github.com/bseverns/benzknob
      - label: Related sketch — MotorLightSound lab
        url: https://github.com/bseverns/MotorLightSound

  - id: glitch-geometry
    title: Glitch Geometry — Audio→Form Instrument
    img_src: /assets/images/cds/glitch-geometry-still.png
    img_alt: Generative geometry frame driven by live audio features
    abstract: >-
      Live translation of signal features into geometry; pipeline choices (feature extraction,
      modulation, rendering) are made legible as aesthetic and ethical decisions. The system is
      tunable and documented for both teaching and critique.
    abstract_locked: true
    aligns: ["digital literacy","justice/representation","DIY"]
    methods:
      - Signal in → features → geometry modulation → render
      - Transparent, documented pipeline for reproducibility
      - Designed for classroom demos and public performance
    outcomes:
      - Live demos (link)
      - Student mini-studies (labs scheduled in October)
    teach:
      goal: Link form to claim via signal features
      lab60: Implement 2 features; compare visual behaviors
      assess: Process log + short critique memo
    links:
      - label: Repository
        url: https://github.com/bseverns/GlitchListener
      - label: README quickstart
        url: https://github.com/bseverns/GlitchListener#readme
      - label: Related sketch — glitchProcessing
        url: https://github.com/bseverns/glitchProcessing
      - label: Related sketch — SonicSketches
        url: https://github.com/bseverns/SonicSketches
      - label: Vimeo excerpt
        url: https://vimeo.com/1120504006

  - id: dead-sky
    title: DEAD SKY — Vision & Motion Grammar Studies
    img_src: /assets/images/cds/ds200801-still.png
    img_alt: Dead Sky rural pursuit still, a lone figure on a November road
    abstract: >-
      Two short studies—rural pursuit (DS200412) and wheel-mounted POV (DS200801)—probing
      surveillance logics, attention, and embodied capture toward a larger film project. Tests
      “pursuit grammar” and mechanical vision’s entrainment.
    abstract_locked: true
    aligns: ["digital literacy","justice/representation"]
    methods:
      - November light, long-lens compression (pursuit grammar)
      - "Wheel POV: cyclic motion & peripheral smear"
      - Consent & site plans for actors/bystanders
    outcomes:
      - Multiple studies completed; Script draft in progress, collaborators identified, schedules tbd.
      - Shoots scheduled for late November 2025
    teach:
      goal: Make camera grammar legible and accountable
      lab60: Long-lens pursuit vs. wheel-POV; reflect on ethics
      assess: Shot log + ethics note + 60s cut
    links:
      - label: "Vimeo — Study DS200412"
        url: https://vimeo.com/459713160
      - label: "Vimeo — Study DS200801"
        url: https://vimeo.com/453387574
      - label: Vimeo profile
        url: https://vimeo.com/user2746012
      - label: "Related tool: VidObjectifier"
        url: https://github.com/bseverns/VidObjectifier

  - id: skyway-derive-media-fast
    title: Skyway Derivé / Media Fast — Critical Pedagogy Interventions
    img_src: /assets/images/cds/spectacle-mediafast.png
    img_alt: Prompt card and public intervention still for Spectacle / Media Fast
    abstract: >-
      Paired interventions that make media power felt: a “Spectacle” action-lecture moves
      critique into public space; a structured “Media Fast” maps sensory shifts and agency
      before reflective media making. Designed as public method; prompts and reflections are
      the artifacts.
    abstract_locked: true
    aligns: ["justice/representation","civic engagement"]
    methods:
      - Transparent prompts; bystander consent & respect
      - Reflection before publication; no IDs without release
      - Documentation pack (brief, roles, reflection prompts)
    outcomes:
      - 1 public (internet) intervention per students
      - 1 compiled reflections doc per students
    teach:
      goal: Embody critique; document before publish
      lab60: Skyway dérive with consent checkpoints
      assess: Reflection + consent notes + debrief
    links:
      - label: Skyway dérive walking score (Media 1/2)
        url: https://github.com/bseverns/Syllabus/blob/main/MCADMedia2/Walking%20in%20the%20Skyway.docx
      - label: Media Fast assignment brief (Media 2)
        url: https://github.com/bseverns/Syllabus/blob/main/MCADMedia2/Media%202%20SP21%20PROJECT%202_%20Media%20Fast.docx
      - label: Media Fast reflection pack (Media 2 folder)
        url: https://github.com/bseverns/Syllabus/blob/main/MCADMedia2/Media%20Fast%20Exercise%20FOLDER/Assignment%20Instructions%20-%20MEDIA%20FAST.docx
      - label: Policies / templates (Syllabus/shared)
        url: https://github.com/bseverns/Syllabus/tree/main/shared

  - id: mcad-media-2
    title: MCAD Media 2 — MTN Public Access Broadcast
    img_src: /assets/images/cds/mcad-media2-mtn.png
    img_alt: "MCAD Media 2: public access broadcast production still"
    abstract: >-
      Studio-seminar culminating in a 28.5-minute MTN broadcast planned, produced, and edited by
      students. The artifact is civic-facing media formed by calendars, critique gates, and
      documentation standards for longevity.
    abstract_locked: true
    aligns: ["civic engagement","digital literacy"]
    methods:
      - Roles & calendars; consent and authorship checkpoints
      - "Deliverables: broadcast master + process docs"
      - Public exhibition as peer review
    outcomes:
      - 28.5-minute episode delivered
      - Airtime / reach - MTN regularly scheduled programming runs videos 4 times per submission
    teach:
      goal: Practice public-facing media with accountability
      lab60: Segment planning → critique gate → deliver
      assess: Checklists + credits + release archive
    links:
      - label: Network link - SPEAK MPLS (formerly Minneapolis Television Network)
        url: https://www.speakmpls.com
      - label: Archived Episode
        url: https://youtu.be/CXDFptRdhJ8
      - label: Syllabus / MCADMedia2
        url: https://github.com/bseverns/Syllabus/tree/main/MCADMedia2
      - label: MEDIA2-codeEXPLAINERS
        url: https://github.com/bseverns/Syllabus/tree/main/MCADMedia2/MEDIA2-codeEXPLAINERS

  - id: genFab
    title: "Generative Fabrication Techniques"
    img_src: /assets/images/cds/genF1.png
    img_alt: "genF1: PETG print of a ribboned isosurface with cellular cavities (three-quarter view)"
    abstract: >-
      Code→form pipeline exploring SDF fields, isosurface extraction, and remeshing.
      <strong>genF1</strong> anchors the series as a physical proof: a PETG print that ran
      <strong>39.5 hours</strong> end-to-end.
    abstract_locked: true
    aligns: ["digital literacy", "DIY"]
    methods:
      - Layered-noise SDF → isosurface (marching cubes)
      - Relax / quad-remesh for printable topology
      - Slice with oriented supports; PETG profile noted; alt-texted documentation
    outcomes:
      - genF1: physical PETG print (39.5 h), photographed and documented
      - genF2–genF3: print-ready meshes with parameter notes and topology comparisons
      - Gallery page with stills; optional 20–40 s turntable clip
    teach:
      goal: >-
        Make the code→form→fabrication chain legible; compare how field frequency and remeshing
        choices alter topology, readability, and print time.
      lab60: >-
        Vary SDF frequency, export a mesh, apply a simple material/shader test, orient for print, and
        record slicer notes; submit 1 annotated still + settings.
      assess: >-
        60% method clarity & reproducibility; 25% form legibility (printability, supports, orientation);
        15% documentation quality (captions, alt text).
    links:
      - label: "View genF gallery"
        url: "/3d/genfab.html"
      - label: "Generative Software"
        url: https://structuresynth.sourceforge.net
      - label: "Slicer profile (PETG)"
        url: "/text/genF1-petg.curaprofile"
